{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Used Cars Analysis Pipeline - Pandas\n",
    "## 02. Data Cleaning and Preprocessing\n",
    "\n",
    "### Objectives:\n",
    "1. Handle missing values with appropriate strategies\n",
    "2. Remove or treat outliers\n",
    "3. Fix data type inconsistencies\n",
    "4. Standardize categorical values\n",
    "5. Create cleaned dataset for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup complete\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '../../'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from src.utils.pandas_utils import (\n",
    "    load_dataset,\n",
    "    print_dataset_summary,\n",
    "    get_dataset_info,\n",
    "    convert_to_datetime,\n",
    "    save_dataframe,\n",
    "    load_config\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: C:\\Users\\Administrator\\Documents\\Luxdev\\used-cars-analysis-pipeline\\data\\raw\\superstore_final_dataset.csv\n",
      "\n",
      "üìä Original Dataset Shape: (9800, 18)\n",
      "\n",
      "======================================================================\n",
      "BEFORE CLEANING SUMMARY\n",
      "======================================================================\n",
      "\n",
      "üìä Dimensions:\n",
      "   ‚Ä¢ Rows: 9,800\n",
      "   ‚Ä¢ Columns: 18\n",
      "   ‚Ä¢ Memory: 9.93 MB\n",
      "\n",
      "üî¢ Data Types:\n",
      "   ‚Ä¢ object: 15 columns\n",
      "   ‚Ä¢ float64: 2 columns\n",
      "   ‚Ä¢ int64: 1 columns\n",
      "\n",
      "‚ùå Missing Data:\n",
      "   ‚Ä¢ Total missing cells: 11\n",
      "   ‚Ä¢ Missing percentage: 0.01%\n",
      "\n",
      "üîÑ Duplicates:\n",
      "   ‚Ä¢ Duplicate rows: 0\n",
      "\n",
      "üìã Column Categories:\n",
      "   ‚Ä¢ Numeric: 3\n",
      "   ‚Ä¢ Categorical: 15\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "config_path = os.path.join(project_root, 'config', 'pipeline_config.yaml')\n",
    "config = load_config(config_path)\n",
    "\n",
    "full_raw_path = os.path.join(project_root, 'data', 'raw')\n",
    "dataset_filename = config['datasets']['superstore']['filename']\n",
    "full_file_path = os.path.join(full_raw_path, dataset_filename)\n",
    "\n",
    "print(f\"Loading dataset: {full_file_path}\")\n",
    "df = pd.read_csv(full_file_path, encoding='ISO-8859-1')\n",
    "\n",
    "print(f\"\\nüìä Original Dataset Shape: {df.shape}\")\n",
    "print_dataset_summary(df, \"Before Cleaning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 1: HANDLING MISSING VALUES\n",
      "======================================================================\n",
      "\n",
      "Missing values before cleaning:\n",
      "Postal_Code    11\n",
      "dtype: int64\n",
      "\n",
      "‚úÖ Missing values after cleaning: 0\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Handle Missing Values\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 1: HANDLING MISSING VALUES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "missing_summary = df.isnull().sum()\n",
    "print(f\"\\nMissing values before cleaning:\")\n",
    "print(missing_summary[missing_summary > 0])\n",
    "\n",
    "# Strategy: Drop columns with >50% missing, impute others\n",
    "threshold = 0.5\n",
    "missing_pct = df.isnull().sum() / len(df)\n",
    "cols_to_drop = missing_pct[missing_pct > threshold].index.tolist()\n",
    "\n",
    "if cols_to_drop:\n",
    "    print(f\"\\n‚ö†Ô∏è  Dropping columns with >{threshold*100}% missing: {cols_to_drop}\")\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "# Fill remaining missing values\n",
    "for col in df.columns:\n",
    "    if df[col].isnull().any():\n",
    "        if df[col].dtype in ['float64', 'int64']:\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "        else:\n",
    "            df[col].fillna(df[col].mode()[0] if not df[col].mode().empty else 'Unknown', inplace=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Missing values after cleaning: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 2: REMOVING DUPLICATES\n",
      "======================================================================\n",
      "\n",
      "Duplicate rows before: 0\n",
      "Duplicate rows after: 0\n",
      "‚úÖ Removed 0 duplicate rows\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Remove Duplicates\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: REMOVING DUPLICATES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "duplicates_before = df.duplicated().sum()\n",
    "print(f\"\\nDuplicate rows before: {duplicates_before:,}\")\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "duplicates_after = df.duplicated().sum()\n",
    "print(f\"Duplicate rows after: {duplicates_after:,}\")\n",
    "print(f\"‚úÖ Removed {duplicates_before - duplicates_after:,} duplicate rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 3: CONVERTING DATE COLUMNS\n",
      "======================================================================\n",
      "\n",
      "Date columns found: ['Order_Date', 'Ship_Date']\n",
      "Converting 'Order_Date' to datetime...\n",
      "   ‚ö†Ô∏è  Warning: 5841 values became NaT\n",
      "   ‚úÖ Converted to datetime\n",
      "Converting 'Ship_Date' to datetime...\n",
      "   ‚ö†Ô∏è  Warning: 5985 values became NaT\n",
      "   ‚úÖ Converted to datetime\n",
      "\n",
      "‚úÖ Date conversions complete\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Convert Date Columns\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 3: CONVERTING DATE COLUMNS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "date_cols = [col for col in df.columns if 'date' in col.lower() or 'Date' in col]\n",
    "print(f\"\\nDate columns found: {date_cols}\")\n",
    "\n",
    "for col in date_cols:\n",
    "    df = convert_to_datetime(df, col)\n",
    "\n",
    "print(\"\\n‚úÖ Date conversions complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 4: HANDLING OUTLIERS\n",
      "======================================================================\n",
      "\n",
      "Numeric columns: ['Row_ID', 'Postal_Code', 'Sales']\n",
      "\n",
      "Sales:\n",
      "  Outliers detected: 1145 (11.68%)\n",
      "  Capping to [-272.79, 500.64]\n",
      "\n",
      "‚úÖ Outlier treatment complete\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Handle Outliers\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 4: HANDLING OUTLIERS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"\\nNumeric columns: {numeric_cols}\")\n",
    "\n",
    "# Cap outliers using IQR method\n",
    "for col in numeric_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers_count = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()\n",
    "    \n",
    "    if outliers_count > 0:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Outliers detected: {outliers_count} ({outliers_count/len(df)*100:.2f}%)\")\n",
    "        print(f\"  Capping to [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "        \n",
    "        df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "print(\"\\n‚úÖ Outlier treatment complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 5: STANDARDIZING CATEGORICAL VARIABLES\n",
      "======================================================================\n",
      "\n",
      "Categorical columns: ['Ship_Mode', 'Customer_Name', 'Segment', 'Country', 'City', 'State', 'Region', 'Category', 'Sub_Category', 'Product_Name']\n",
      "\n",
      "‚úÖ Categorical standardization complete\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Standardize Categorical Variables\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 5: STANDARDIZING CATEGORICAL VARIABLES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "categorical_cols = [c for c in categorical_cols if not any(x in c.lower() for x in ['date', 'id'])]\n",
    "\n",
    "print(f\"\\nCategorical columns: {categorical_cols}\")\n",
    "\n",
    "for col in categorical_cols:\n",
    "    # Strip whitespace\n",
    "    df[col] = df[col].str.strip()\n",
    "    \n",
    "    # Standardize case (title case)\n",
    "    unique_before = df[col].nunique()\n",
    "    df[col] = df[col].str.title()\n",
    "    unique_after = df[col].nunique()\n",
    "    \n",
    "    if unique_before != unique_after:\n",
    "        print(f\"  {col}: {unique_before} ‚Üí {unique_after} unique values\")\n",
    "\n",
    "print(\"\\n‚úÖ Categorical standardization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CLEANING SUMMARY\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "AFTER CLEANING SUMMARY\n",
      "======================================================================\n",
      "\n",
      "üìä Dimensions:\n",
      "   ‚Ä¢ Rows: 9,800\n",
      "   ‚Ä¢ Columns: 18\n",
      "   ‚Ä¢ Memory: 8.86 MB\n",
      "\n",
      "üî¢ Data Types:\n",
      "   ‚Ä¢ object: 13 columns\n",
      "   ‚Ä¢ datetime64[ns]: 2 columns\n",
      "   ‚Ä¢ float64: 2 columns\n",
      "   ‚Ä¢ int64: 1 columns\n",
      "\n",
      "‚ùå Missing Data:\n",
      "   ‚Ä¢ Total missing cells: 11,826\n",
      "   ‚Ä¢ Missing percentage: 6.70%\n",
      "\n",
      "üîÑ Duplicates:\n",
      "   ‚Ä¢ Duplicate rows: 0\n",
      "\n",
      "üìã Column Categories:\n",
      "   ‚Ä¢ Numeric: 3\n",
      "   ‚Ä¢ Categorical: 13\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "üìä Cleaning Impact:\n",
      "  Missing values: 0 (was: 11826)\n",
      "  Duplicates: 0\n",
      "  Shape: (9800, 18)\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Create Cleaned Dataset Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLEANING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print_dataset_summary(df, \"After Cleaning\")\n",
    "\n",
    "# Compare before/after\n",
    "print(\"\\nüìä Cleaning Impact:\")\n",
    "print(f\"  Missing values: 0 (was: {df.isnull().sum().sum()})\")\n",
    "print(f\"  Duplicates: 0\")\n",
    "print(f\"  Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SAVING CLEANED DATASET\n",
      "======================================================================\n",
      "‚úÖ Saved to: C:\\Users\\Administrator\\Documents\\Luxdev\\used-cars-analysis-pipeline\\data\\cleaned\\superstore_cleaned.csv\n",
      "‚úÖ Saved to: C:\\Users\\Administrator\\Documents\\Luxdev\\used-cars-analysis-pipeline\\data\\cleaned\\superstore_cleaned.parquet\n",
      "\n",
      "‚úÖ Data cleaning pipeline complete!\n",
      "\n",
      "üìÅ Cleaned data saved to: C:\\Users\\Administrator\\Documents\\Luxdev\\used-cars-analysis-pipeline\\data\\cleaned\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Save Cleaned Dataset\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAVING CLEANED DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "output_dir = os.path.join(project_root, 'data', 'cleaned')\n",
    "save_dataframe(df, 'superstore_cleaned.csv', output_dir, format='csv')\n",
    "save_dataframe(df, 'superstore_cleaned.parquet', output_dir, format='parquet')\n",
    "\n",
    "print(\"\\n‚úÖ Data cleaning pipeline complete!\")\n",
    "print(f\"\\nüìÅ Cleaned data saved to: {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
