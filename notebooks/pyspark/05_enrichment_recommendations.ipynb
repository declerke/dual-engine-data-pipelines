{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cfdbde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Setup complete: Java 17.0.17.10 is now mapped.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Set the specific path you found\n",
    "os.environ['JAVA_HOME'] = r'C:\\Program Files\\Eclipse Adoptium\\jdk-17.0.17.10-hotspot'\n",
    "os.environ['HADOOP_HOME'] = r'C:\\hadoop'\n",
    "\n",
    "# Update the system path so Python finds the correct java.exe and winutils.exe\n",
    "os.environ['PATH'] = (\n",
    "    os.path.join(os.environ['JAVA_HOME'], 'bin') + os.pathsep + \n",
    "    os.path.join(os.environ['HADOOP_HOME'], 'bin') + os.pathsep + \n",
    "    os.environ['PATH']\n",
    ")\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, concat, monotonically_increasing_id\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '../../'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from src.utils.spark_utils import create_spark_session, stop_spark_session\n",
    "\n",
    "print(\"âœ… Setup complete: Java 17.0.17.10 is now mapped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a9e9077",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ SUCCESS: Spark session is active!\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Enrichment Analysis\") \\\n",
    "        .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "        .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "        .config(\"spark.sql.sources.commitProtocolClass\", \"org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    print(\"ğŸ”¥ SUCCESS: Spark session is active!\")\n",
    "    spark.range(3).show()\n",
    "except Exception as e:\n",
    "    print(\"âŒ Connection still blocked. Checking logs...\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dcd5e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Dataset Assessment\n",
      "======================================================================\n",
      "\n",
      "Rows: 9,800\n",
      "Columns: 18\n",
      "\n",
      "Existing columns: ['Row_ID', 'Order_ID', 'Order_Date', 'Ship_Date', 'Ship_Mode', 'Customer_ID', 'Customer_Name', 'Segment', 'Country', 'City', 'State', 'Postal_Code', 'Region', 'Product_ID', 'Category', 'Sub_Category', 'Product_Name', 'Sales']\n"
     ]
    }
   ],
   "source": [
    "cleaned_path = os.path.join(project_root, 'data', 'cleaned', 'superstore_cleaned_spark.parquet')\n",
    "df = spark.read.parquet(cleaned_path)\n",
    "\n",
    "print(f\"ğŸ“Š Dataset Assessment\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "print(f\"Rows: {df.count():,}\")\n",
    "print(f\"Columns: {len(df.columns)}\")\n",
    "print(f\"\\nExisting columns: {df.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19bbffe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ SCALABLE DATA ENRICHMENT STRATEGIES\n",
      "======================================================================\n",
      "\n",
      "ğŸŒ 1. GEOGRAPHIC ENRICHMENT AT SCALE\n",
      "   Challenge: Geocoding millions of addresses\n",
      "   PySpark Solution: Broadcast unique location lookups.\n",
      "\n",
      "ğŸ‘¥ 2. DEMOGRAPHIC DATA ENRICHMENT\n",
      "   Challenge: Adding census/population data\n",
      "   PySpark Solution: Join on geographic keys using partitioned Parquet files.\n",
      "\n",
      "ğŸŒ¤ï¸ 3. WEATHER DATA ENRICHMENT\n",
      "   Challenge: Historical weather for each order\n",
      "   PySpark Solution: Join on date + location keys using Delta Lake for ACID consistency.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nğŸ“ SCALABLE DATA ENRICHMENT STRATEGIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "enrichment_strategies = \"\"\"\n",
    "ğŸŒ 1. GEOGRAPHIC ENRICHMENT AT SCALE\n",
    "   Challenge: Geocoding millions of addresses\n",
    "   PySpark Solution: Broadcast unique location lookups.\n",
    "\n",
    "ğŸ‘¥ 2. DEMOGRAPHIC DATA ENRICHMENT\n",
    "   Challenge: Adding census/population data\n",
    "   PySpark Solution: Join on geographic keys using partitioned Parquet files.\n",
    "\n",
    "ğŸŒ¤ï¸ 3. WEATHER DATA ENRICHMENT\n",
    "   Challenge: Historical weather for each order\n",
    "   PySpark Solution: Join on date + location keys using Delta Lake for ACID consistency.\n",
    "\"\"\"\n",
    "print(enrichment_strategies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "429390f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ—ï¸ STAR SCHEMA DESIGN - BIG DATA OPTIMIZED\n",
      "======================================================================\n",
      "\n",
      "ğŸ”‘ Big Data Optimizations:\n",
      "\n",
      "1. PARTITIONING STRATEGY\n",
      "   Fact table: Partition by date (year/month).\n",
      "   Dimensions: Partition by region/category.\n",
      "\n",
      "2. FILE FORMAT\n",
      "   Use Parquet or Delta Lake for columnar storage.\n",
      "\n",
      "3. BUCKETING\n",
      "   Bucket fact table by high-cardinality keys to reduce shuffles.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nğŸ—ï¸ STAR SCHEMA DESIGN - BIG DATA OPTIMIZED\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "schema_design = \"\"\"\n",
    "ğŸ”‘ Big Data Optimizations:\n",
    "\n",
    "1. PARTITIONING STRATEGY\n",
    "   Fact table: Partition by date (year/month).\n",
    "   Dimensions: Partition by region/category.\n",
    "\n",
    "2. FILE FORMAT\n",
    "   Use Parquet or Delta Lake for columnar storage.\n",
    "\n",
    "3. BUCKETING\n",
    "   Bucket fact table by high-cardinality keys to reduce shuffles.\n",
    "\"\"\"\n",
    "print(schema_design)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0df3508e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¨ BUILDING DIMENSION TABLES WITH PYSPARK\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "print(\"\\nğŸ”¨ BUILDING DIMENSION TABLES WITH PYSPARK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if 'Customer Name' in df.columns and 'Segment' in df.columns:\n",
    "    dim_customer = df.select('Customer Name', 'Segment', 'Region').distinct() \\\n",
    "        .withColumn('customer_key', monotonically_increasing_id())\n",
    "    \n",
    "    print(f\"    âœ… Created dim_customer with {dim_customer.count():,} unique customers\")\n",
    "    dim_customer.show(5)\n",
    "\n",
    "if 'Product Name' in df.columns and 'Category' in df.columns:\n",
    "    dim_product = df.select('Product Name', 'Category', 'Sub-Category').distinct() \\\n",
    "        .withColumn('product_key', monotonically_increasing_id())\n",
    "    \n",
    "    print(f\"    âœ… Created dim_product with {dim_product.count():,} unique products\")\n",
    "    dim_product.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e436dafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ DISTRIBUTED ETL PIPELINE ARCHITECTURE\n",
      "======================================================================\n",
      "\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚                TRANSFORM PHASE (Distributed)                â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ 1. Data Cleaning (Parallel Operations)                      â”‚\n",
      "â”‚ 2. Data Enrichment (Batch API calls)                        â”‚\n",
      "â”‚ 3. Dimensional Modeling (Surrogate Key generation)          â”‚\n",
      "â”‚ 4. Aggregations (Window functions)                          â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nğŸ”„ DISTRIBUTED ETL PIPELINE ARCHITECTURE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "etl_architecture = \"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                TRANSFORM PHASE (Distributed)                â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ 1. Data Cleaning (Parallel Operations)                      â”‚\n",
    "â”‚ 2. Data Enrichment (Batch API calls)                        â”‚\n",
    "â”‚ 3. Dimensional Modeling (Surrogate Key generation)          â”‚\n",
    "â”‚ 4. Aggregations (Window functions)                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\"\n",
    "print(etl_architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ce97303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’» PYSPARK ETL CODE PATTERN\n",
      "======================================================================\n",
      "\n",
      "# (Conceptual Pattern)\n",
      "df_clean = df_raw.dropDuplicates().fillna(0)\n",
      "\n",
      "dim_customer = df_clean.select(\"customer_id\", \"customer_name\").distinct()     .withColumn(\"customer_key\", monotonically_increasing_id())\n",
      "\n",
      "fact_sales = df_clean.join(broadcast(dim_customer), on=\"customer_id\", how=\"inner\")\n",
      "\n",
      "fact_sales.write.format(\"parquet\").mode(\"append\").partitionBy(\"year\", \"month\").save(path)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nğŸ’» PYSPARK ETL CODE PATTERN\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "code_example = '''\n",
    "# (Conceptual Pattern)\n",
    "df_clean = df_raw.dropDuplicates().fillna(0)\n",
    "\n",
    "dim_customer = df_clean.select(\"customer_id\", \"customer_name\").distinct() \\\n",
    "    .withColumn(\"customer_key\", monotonically_increasing_id())\n",
    "\n",
    "fact_sales = df_clean.join(broadcast(dim_customer), on=\"customer_id\", how=\"inner\")\n",
    "\n",
    "fact_sales.write.format(\"parquet\").mode(\"append\").partitionBy(\"year\", \"month\").save(path)\n",
    "'''\n",
    "print(code_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff721e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Spark session stopped\n",
      "\n",
      "âœ… Analysis complete! You have successfully built a scalable PySpark pipeline.\n"
     ]
    }
   ],
   "source": [
    "if 'spark' in locals():\n",
    "    spark.stop()\n",
    "    print(\"âœ… Spark session stopped\")\n",
    "\n",
    "print(\"\\nâœ… Analysis complete! You have successfully built a scalable PySpark pipeline.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
