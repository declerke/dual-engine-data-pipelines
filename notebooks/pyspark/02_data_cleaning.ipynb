{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Used Cars Analysis Pipeline - PySpark\n",
    "## 02. Data Cleaning and Preprocessing\n",
    "\n",
    "### Objectives:\n",
    "1. Handle missing values with PySpark\n",
    "2. Remove duplicates at scale\n",
    "3. Fix data type inconsistencies\n",
    "4. Standardize categorical values\n",
    "5. Compare performance with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Spark Session initialized for Cleaning using config memory: 4g\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import findspark\n",
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '../../'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "config_path = os.path.join(project_root, 'config', 'pipeline_config.yaml')\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "hadoop_home = os.path.join(project_root, 'bin', 'winutils')\n",
    "os.environ['HADOOP_HOME'] = hadoop_home\n",
    "os.environ['PATH'] += os.pathsep + os.path.join(hadoop_home, 'bin')\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "spark_memory = config.get('spark', {}).get('memory', {})\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Cleaning_PySpark\") \\\n",
    "    .config(\"spark.driver.memory\", spark_memory.get('driver', '2g')) \\\n",
    "    .config(\"spark.executor.memory\", spark_memory.get('executor', '2g')) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"âœ… Spark Session initialized for Cleaning using config memory: {spark_memory.get('driver')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: C:\\Users\\Administrator\\Documents\\Luxdev\\used-cars-analysis-pipeline\\data\\raw\\superstore_final_dataset.csv\n",
      "\n",
      "ðŸ“Š Dataset loaded in 4.01 seconds\n",
      "Rows: 9,800\n",
      "Columns: 18\n",
      "\n",
      "Schema:\n",
      "root\n",
      " |-- Row_ID: integer (nullable = true)\n",
      " |-- Order_ID: string (nullable = true)\n",
      " |-- Order_Date: string (nullable = true)\n",
      " |-- Ship_Date: string (nullable = true)\n",
      " |-- Ship_Mode: string (nullable = true)\n",
      " |-- Customer_ID: string (nullable = true)\n",
      " |-- Customer_Name: string (nullable = true)\n",
      " |-- Segment: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Postal_Code: integer (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Product_ID: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Sub_Category: string (nullable = true)\n",
      " |-- Product_Name: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "raw_filename = config['datasets']['superstore']['filename']\n",
    "raw_path = os.path.join(project_root, 'data', 'raw', raw_filename)\n",
    "\n",
    "print(f\"Loading dataset: {raw_path}\")\n",
    "start_time = time.time()\n",
    "\n",
    "df = spark.read.csv(\n",
    "    raw_path,\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    encoding='ISO-8859-1'\n",
    ")\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nðŸ“Š Dataset loaded in {load_time:.2f} seconds\")\n",
    "print(f\"Rows: {df.count():,}\")\n",
    "print(f\"Columns: {len(df.columns)}\")\n",
    "\n",
    "print(\"\\nSchema:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 1: ANALYZING MISSING VALUES\n",
      "======================================================================\n",
      "\n",
      "Missing values by column:\n",
      "  Postal_Code                          11 (  0.11%)\n",
      "  Row_ID                                0 (  0.00%)\n",
      "  Order_ID                              0 (  0.00%)\n",
      "  Order_Date                            0 (  0.00%)\n",
      "  Ship_Date                             0 (  0.00%)\n",
      "  Ship_Mode                             0 (  0.00%)\n",
      "  Customer_ID                           0 (  0.00%)\n",
      "  Customer_Name                         0 (  0.00%)\n",
      "  Segment                               0 (  0.00%)\n",
      "  Country                               0 (  0.00%)\n",
      "  City                                  0 (  0.00%)\n",
      "  State                                 0 (  0.00%)\n",
      "  Region                                0 (  0.00%)\n",
      "  Product_ID                            0 (  0.00%)\n",
      "  Category                              0 (  0.00%)\n",
      "  Sub_Category                          0 (  0.00%)\n",
      "  Product_Name                          0 (  0.00%)\n",
      "  Sales                                 0 (  0.00%)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 1: ANALYZING MISSING VALUES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "null_counts = df.select(\n",
    "    [count(when(col(c).isNull(), c)).alias(c) for c in df.columns]\n",
    ").collect()[0].asDict()\n",
    "\n",
    "total_rows = df.count()\n",
    "\n",
    "print(\"\\nMissing values by column:\")\n",
    "for col_name, null_count in sorted(null_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    if null_count > 0:\n",
    "        pct = (null_count / total_rows) * 100\n",
    "        print(f\"  {col_name:<30} {null_count:>8,} ({pct:>6.2f}%)\")\n",
    "    else:\n",
    "        print(f\"  {col_name:<30}        0 (  0.00%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 2: HANDLING MISSING VALUES\n",
      "======================================================================\n",
      "\n",
      "âœ… No columns exceed missing threshold\n",
      "\n",
      "Filling missing values:\n",
      "  Numeric columns: 0\n",
      "  Categorical columns: 0\n",
      "\n",
      "âœ… Missing values handled\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Handle Missing Values\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: HANDLING MISSING VALUES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Drop columns with >50% missing\n",
    "threshold = 0.5\n",
    "cols_to_drop = []\n",
    "\n",
    "for col_name, null_count in null_counts.items():\n",
    "    if (null_count / total_rows) > threshold:\n",
    "        cols_to_drop.append(col_name)\n",
    "\n",
    "if cols_to_drop:\n",
    "    print(f\"\\nâš ï¸  Dropping columns with >{threshold*100}% missing: {cols_to_drop}\")\n",
    "    df = df.drop(*cols_to_drop)\n",
    "else:\n",
    "    print(\"\\nâœ… No columns exceed missing threshold\")\n",
    "\n",
    "# Fill missing values\n",
    "# For numeric columns: fill with median\n",
    "# For categorical: fill with mode or 'Unknown'\n",
    "\n",
    "numeric_cols = [f.name for f in df.schema.fields if str(f.dataType) in ['IntegerType', 'DoubleType', 'FloatType', 'LongType']]\n",
    "categorical_cols = [f.name for f in df.schema.fields if str(f.dataType) == 'StringType']\n",
    "\n",
    "print(f\"\\nFilling missing values:\")\n",
    "print(f\"  Numeric columns: {len(numeric_cols)}\")\n",
    "print(f\"  Categorical columns: {len(categorical_cols)}\")\n",
    "\n",
    "# Calculate medians for numeric columns\n",
    "fill_values = {}\n",
    "\n",
    "for col_name in numeric_cols:\n",
    "    median_val = df.approxQuantile(col_name, [0.5], 0.01)[0]\n",
    "    fill_values[col_name] = median_val\n",
    "\n",
    "# Fill with calculated values\n",
    "df = df.fillna(fill_values)\n",
    "\n",
    "# Fill categorical with 'Unknown'\n",
    "df = df.fillna('Unknown', subset=categorical_cols)\n",
    "\n",
    "print(\"\\nâœ… Missing values handled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 3: REMOVING DUPLICATES\n",
      "======================================================================\n",
      "\n",
      "Rows before: 9,800\n",
      "Rows after: 9,800\n",
      "âœ… Removed 0 duplicate rows\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Remove Duplicates\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 3: REMOVING DUPLICATES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "rows_before = df.count()\n",
    "df = df.dropDuplicates()\n",
    "rows_after = df.count()\n",
    "\n",
    "duplicates_removed = rows_before - rows_after\n",
    "\n",
    "print(f\"\\nRows before: {rows_before:,}\")\n",
    "print(f\"Rows after: {rows_after:,}\")\n",
    "print(f\"âœ… Removed {duplicates_removed:,} duplicate rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 4: CONVERTING DATE COLUMNS\n",
      "======================================================================\n",
      "  âœ… Converted Order_Date using format d/M/yyyy\n",
      "  âœ… Converted Ship_Date using format d/M/yyyy\n",
      "\n",
      "âœ… Date conversions complete\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 4: CONVERTING DATE COLUMNS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "date_cols = [c for c in df.columns if 'date' in c.lower()]\n",
    "\n",
    "for col_name in date_cols:\n",
    "    df = df.withColumn(col_name, to_date(col(col_name), 'd/M/yyyy'))\n",
    "    print(f\"  âœ… Converted {col_name} using format d/M/yyyy\")\n",
    "\n",
    "print(\"\\nâœ… Date conversions complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 5: HANDLING OUTLIERS\n",
      "======================================================================\n",
      "\n",
      "Numeric columns: []\n",
      "\n",
      "âœ… Outlier treatment complete\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Handle Outliers using IQR method\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 5: HANDLING OUTLIERS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nNumeric columns: {numeric_cols}\")\n",
    "\n",
    "for col_name in numeric_cols:\n",
    "    # Calculate quartiles\n",
    "    quartiles = df.approxQuantile(col_name, [0.25, 0.75], 0.01)\n",
    "    Q1, Q3 = quartiles[0], quartiles[1]\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Count outliers\n",
    "    outliers = df.filter((col(col_name) < lower_bound) | (col(col_name) > upper_bound)).count()\n",
    "    \n",
    "    if outliers > 0:\n",
    "        print(f\"\\n{col_name}:\")\n",
    "        print(f\"  Outliers detected: {outliers:,} ({outliers/rows_after*100:.2f}%)\")\n",
    "        print(f\"  Capping to [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "        \n",
    "        # Cap outliers\n",
    "        df = df.withColumn(\n",
    "            col_name,\n",
    "            when(col(col_name) < lower_bound, lower_bound)\n",
    "            .when(col(col_name) > upper_bound, upper_bound)\n",
    "            .otherwise(col(col_name))\n",
    "        )\n",
    "\n",
    "print(\"\\nâœ… Outlier treatment complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 6: STANDARDIZING CATEGORICAL VARIABLES\n",
      "======================================================================\n",
      "\n",
      "Categorical columns to standardize: []\n",
      "\n",
      "âœ… Categorical standardization complete\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Standardize Categorical Variables\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 6: STANDARDIZING CATEGORICAL VARIABLES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Exclude date and ID columns\n",
    "cat_cols_to_clean = [\n",
    "    c for c in categorical_cols \n",
    "    if not any(x in c.lower() for x in ['date', 'id'])\n",
    "]\n",
    "\n",
    "print(f\"\\nCategorical columns to standardize: {cat_cols_to_clean}\")\n",
    "\n",
    "for col_name in cat_cols_to_clean:\n",
    "    unique_before = df.select(col_name).distinct().count()\n",
    "    \n",
    "    # Trim whitespace and convert to title case\n",
    "    df = df.withColumn(col_name, initcap(trim(col(col_name))))\n",
    "    \n",
    "    unique_after = df.select(col_name).distinct().count()\n",
    "    \n",
    "    if unique_before != unique_after:\n",
    "        print(f\"  {col_name}: {unique_before} â†’ {unique_after} unique values\")\n",
    "\n",
    "print(\"\\nâœ… Categorical standardization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CLEANING SUMMARY\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š Final Dataset:\n",
      "  Rows: 9,800\n",
      "  Columns: 18\n",
      "  Missing values: 11\n",
      "  Duplicates: 0\n",
      "\n",
      "âœ… Data cleaning pipeline complete!\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Validation Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLEANING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "final_count = df.count()\n",
    "\n",
    "# Check for remaining nulls\n",
    "null_check = df.select(\n",
    "    [count(when(col(c).isNull(), c)).alias(c) for c in df.columns]\n",
    ").collect()[0].asDict()\n",
    "\n",
    "total_nulls = sum(null_check.values())\n",
    "\n",
    "print(f\"\\nðŸ“Š Final Dataset:\")\n",
    "print(f\"  Rows: {final_count:,}\")\n",
    "print(f\"  Columns: {len(df.columns)}\")\n",
    "print(f\"  Missing values: {total_nulls}\")\n",
    "print(f\"  Duplicates: 0\")\n",
    "\n",
    "print(\"\\nâœ… Data cleaning pipeline complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SAVING CLEANED DATASET\n",
      "======================================================================\n",
      "âœ… Saved to: C:\\Users\\Administrator\\Documents\\Luxdev\\used-cars-analysis-pipeline\\data\\cleaned\\superstore_cleaned_spark.parquet\n",
      "âœ… Saved to: C:\\Users\\Administrator\\Documents\\Luxdev\\used-cars-analysis-pipeline\\data\\cleaned\\superstore_cleaned_spark.csv\n",
      "\n",
      "ðŸ“ Cleaned data saved in multiple formats\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAVING CLEANED DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "spark.conf.set(\"spark.sql.sources.commitProtocolClass\", \"org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol\")\n",
    "spark.conf.set(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\")\n",
    "\n",
    "output_dir = os.path.join(project_root, 'data', 'cleaned')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "parquet_path = os.path.join(output_dir, 'superstore_cleaned_spark.parquet')\n",
    "df.write.mode('overwrite').parquet(parquet_path)\n",
    "print(f\"âœ… Saved to: {parquet_path}\")\n",
    "\n",
    "csv_path = os.path.join(output_dir, 'superstore_cleaned_spark.csv')\n",
    "df.coalesce(1).write.mode('overwrite').option('header', 'true').csv(csv_path)\n",
    "print(f\"âœ… Saved to: {csv_path}\")\n",
    "\n",
    "print(\"\\nðŸ“ Cleaned data saved in multiple formats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“‹ Sample of cleaned data:\n",
      "+------+--------------+----------+----------+--------------+-----------+-----------------+-----------+-------------+-------------+--------------+-----------+-------+---------------+---------------+------------+---------------------------------------------------------------------+-------+\n",
      "|Row_ID|Order_ID      |Order_Date|Ship_Date |Ship_Mode     |Customer_ID|Customer_Name    |Segment    |Country      |City         |State         |Postal_Code|Region |Product_ID     |Category       |Sub_Category|Product_Name                                                         |Sales  |\n",
      "+------+--------------+----------+----------+--------------+-----------+-----------------+-----------+-------------+-------------+--------------+-----------+-------+---------------+---------------+------------+---------------------------------------------------------------------+-------+\n",
      "|67    |US-2016-164175|2016-04-30|2016-05-05|Standard Class|PS-18970   |Paul Stevenson   |Home Office|United States|Chicago      |Illinois      |60610      |Central|FUR-CH-10001146|Furniture      |Chairs      |Global Value Mid-Back Managers Chair, Gray                           |213.115|\n",
      "|112   |CA-2017-128867|2017-11-03|2017-11-10|Standard Class|CL-12565   |Clay Ludtke      |Consumer   |United States|Urbandale    |Iowa          |50322      |Central|OFF-AR-10000380|Office Supplies|Art         |Hunt PowerHouse Electric Pencil Sharpener, Blue                      |75.96  |\n",
      "|353   |CA-2017-129714|2017-09-01|2017-09-03|First Class   |AB-10060   |Adam Bellavance  |Home Office|United States|New York City|New York      |10009      |East   |OFF-PA-10001970|Office Supplies|Paper       |Xerox 1881                                                           |49.12  |\n",
      "|511   |CA-2018-135307|2018-11-26|2018-11-27|First Class   |LS-17245   |Lynn Smith       |Consumer   |United States|Gladstone    |Missouri      |64118      |Central|FUR-FU-10001290|Furniture      |Furnishings |Executive Impressions Supervisor Wall Clock                          |126.3  |\n",
      "|793   |CA-2017-105256|2017-05-20|2017-05-20|Same Day      |JK-15730   |Joe Kamberova    |Consumer   |United States|Asheville    |North Carolina|28806      |South  |TEC-PH-10001530|Technology     |Phones      |Cisco Unified IP Phone 7945G VoIP phone                              |1363.96|\n",
      "|822   |CA-2015-140858|2015-06-28|2015-07-02|Standard Class|CA-12775   |Cynthia Arntzen  |Consumer   |United States|Philadelphia |Pennsylvania  |19140      |East   |OFF-PA-10003395|Office Supplies|Paper       |Xerox 1941                                                           |335.52 |\n",
      "|952   |US-2018-110576|2018-11-28|2018-12-02|Standard Class|RB-19795   |Ross Baird       |Home Office|United States|Philadelphia |Pennsylvania  |19120      |East   |OFF-PA-10002479|Office Supplies|Paper       |Xerox 4200 Series MultiUse Premium Copy Paper (20Lb. and 84 Bright)  |25.344 |\n",
      "|1167  |CA-2016-125416|2016-11-02|2016-11-02|Same Day      |KC-16540   |Kelly Collister  |Consumer   |United States|Seattle      |Washington    |98115      |West   |TEC-AC-10001552|Technology     |Accessories |Logitech K350 2.4Ghz Wireless Keyboard                               |447.93 |\n",
      "|1170  |CA-2018-145226|2018-12-08|2018-12-10|Second Class  |DL-13315   |Delfina Latchford|Consumer   |United States|New York City|New York      |10035      |East   |OFF-PA-10003172|Office Supplies|Paper       |Xerox 1996                                                           |19.44  |\n",
      "|1306  |CA-2017-101966|2017-07-14|2017-07-16|Second Class  |BM-11785   |Bryan Mills      |Consumer   |United States|Houston      |Texas         |77036      |Central|TEC-PH-10003437|Technology     |Phones      |Blue Parrot B250XT Professional Grade Wireless BluetoothÂ HeadsetÂ with|419.944|\n",
      "+------+--------------+----------+----------+--------------+-----------+-----------------+-----------+-------------+-------------+--------------+-----------+-------+---------------+---------------+------------+---------------------------------------------------------------------+-------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Show sample of cleaned data\n",
    "print(\"\\nðŸ“‹ Sample of cleaned data:\")\n",
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop_spark_session' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m-----------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Stop Spark session\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mstop_spark_session\u001b[49m(spark)\n",
      "\u001b[31mNameError\u001b[39m: name 'stop_spark_session' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "print(\"âœ… Spark session stopped successfully\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
