# ğŸš€ Dual-Engine Data Pipelines: Pandas vs. PySpark

A professional data engineering repository benchmarking Pandas and PySpark across high-scale Retail (Superstore) and Real Estate (Airbnb) datasets. This project demonstrates the transition from local exploratory analysis to a distributed, production-grade Data Warehouse.



---

## ğŸ¯ Project Overview
This pipeline implements identical business logic across two processing engines to evaluate performance, code complexity, and scalability. It features automated data quality audits and advanced time-series forecasting.

**Pipeline Stages:**
* **Data Ingestion:** Loading raw CSV sources into a tiered Data Lake (Raw â†’ Processed â†’ Cleaned).
* **Automated Profiling:** Generating comprehensive HTML reports via `ydata-profiling`.
* **Data Quality Audit:** Enforcing schema and business rules using **Great Expectations**.
* **Time Series Modeling:** Forecasting sales and pricing trends with **Prophet** and **ARIMA**.
* **Warehouse Design:** Designing OLAP-ready **Star Schemas** (Fact & Dimension tables) stored in **Delta Lake** / **Parquet**.

---

## ğŸ—ï¸ Project Structure
```text
dual-engine-data-pipelines/
â”œâ”€â”€ config/               # Pipeline and Spark configuration (YAML)
â”œâ”€â”€ data/                 # Tiered storage: Raw, Processed, and Cleaned
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ pandas/           # Pandas-based processing (01-05)
â”‚   â””â”€â”€ pyspark/          # PySpark-based processing (01-05)
â”œâ”€â”€ reports/              # Generated Artifacts
â”‚   â”œâ”€â”€ figures/          # Forecast plots and correlation heatmaps
â”‚   â””â”€â”€ html/             # ydata-profiling and Quality Audit reports
â”œâ”€â”€ src/                  # Modular pipeline logic
â”œâ”€â”€ tests/                # Unit tests and validation scripts
â”œâ”€â”€ bin/                  # Helper binaries and environment scripts
â””â”€â”€ environment.yml       # Conda environment definition
```

---

## âš¡ Framework Comparison Matrix

| Feature | Pandas | PySpark |
| :--- | :--- | :--- |
| **Best For** | < 10GB Data | > 10GB Data |
| **Processing** | Single-machine / Eager | Distributed / Lazy |
| **Optimization** | Vectorization | Catalyst & Tungsten Engine |
| **Scalability** | Vertical (RAM bound) | Horizontal (Scalable Nodes) |

---

## ğŸ› ï¸ Technical Stack

| Category | Tools |
| :--- | :--- |
| **Core Processing** | PySpark 4.1.1, Pandas 2.2.0, NumPy, PyArrow |
| **Quality & Profiling** | Great Expectations, ydata-profiling |
| **Time Series** | Prophet, Statsmodels, pmdarima |
| **Visualization** | Matplotlib, Seaborn, Plotly |
| **Database/Storage** | Delta Lake, SQLAlchemy, Parquet |

---

## ğŸ”§ Installation & Setup

### Prerequisites
* Python 3.11
* Java 17 (Required for Spark 4.1.1)
* Hadoop Home (`winutils.exe` in `C:\hadoop\bin`)

### Quick Start
```powershell
conda env create -f environment.yml
conda activate spark-project
python src/utils/spark_utils.py
```

### Windows-Specific Optimization
To resolve the `JAVA_GATEWAY_EXITED` error and ensure stable communication on Windows, the Spark Session uses forced loopback binding:



```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .config("spark.driver.host", "127.0.0.1") \
    .config("spark.driver.bindAddress", "127.0.0.1") \
    .getOrCreate()
```

---

## ğŸ“Š Analytics & Reporting
The pipeline automatically populates the `reports/` directory:
* **Automated Audits:** `reports/html/` contains interactive profiling and data validation results from Great Expectations.
* **Forecasting Visuals:** `reports/figures/` stores trend analysis generated by Prophet and Seaborn.

---

## ğŸ“ Skills Demonstrated
* **Data Engineering:** Star Schema design, ETL orchestration, and **Parquet/Delta Lake** optimization.
* **Scalable Analytics:** Distributed computing with PySpark and lazy evaluation tuning.
* **Data Governance:** Validating data integrity using **Great Expectations**.
* **Forecasting:** Building scalable time-series models for business intelligence.
